{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mysql.connector.connection.MySQLConnection object at 0x7fde4ab9c090>\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector as mysql\n",
    "import json\n",
    "\n",
    "with open(\"./mysql_config/config.json\", \"r\") as credential_handle:\n",
    "    sql_creds = json.load(credential_handle)\n",
    "\n",
    "db = mysql.connect(\n",
    "    host     = sql_creds['host'],\n",
    "    user     = sql_creds['username'],\n",
    "    password = sql_creds['password'],\n",
    "    use_pure = True,\n",
    "    database = \"sportfishing\"\n",
    ")\n",
    "\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_rows(page_number):\n",
    "    base_url = \"https://www.fishreports.com/embed/independence-fishreports.php?page={}\"\n",
    "    page     = requests.get(base_url.format(page_number))\n",
    "    soup     = BeautifulSoup(page.text)\n",
    "    # get table rows (these contain fish report)\n",
    "    rows     = soup.find_all('tr')\n",
    "    \n",
    "    # trim header and footer, even if the page has less than 10 posts\n",
    "    rows = rows[2:]\n",
    "    rows = rows[:-1]\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fish_report(report_row):\n",
    "    # get needed data from row\n",
    "    date, description = report_row.find_all('td')\n",
    "    date              = date.text\n",
    "    main_post_link    = description.find_all('a', href=True)[0].get('href', None).replace(\"\\\\\", \"/\")\n",
    "    \n",
    "    # grab the main post and extract its post text\n",
    "    main_post_content = requests.get(main_post_link)\n",
    "    main_post         = BeautifulSoup(main_post_content.text)\n",
    "    headline          = main_post.find_all(\"p\", attrs={'class': \"text-center lead\"})[0].text\n",
    "    \n",
    "    main_post_text    = main_post.find_all(\"div\", attrs={\"class\": \"report_descript_data\"})[0] \n",
    "    \n",
    "    # not all paragraphs are in a <p> tag for whatever reason... hack it!\n",
    "    try:\n",
    "        main_post_text = main_post_text.p.text.strip()\n",
    "    except AttributeError:\n",
    "        main_post_text = main_post_text.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract text body for {headline}, date {date}\")\n",
    "        raise e\n",
    "\n",
    "    return date, headline, main_post_link, main_post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rows = get_table_rows(page_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_fish_report(data_rows[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date, headline, main_post_link, main_post_text = extract_fish_report(data_rows[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add fishing reports to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor = db.cursor()\n",
    "\n",
    "# cursor.execute(\"SHOW DATABASES\")\n",
    "\n",
    "# ## 'fetchall()' method fetches all the rows from the last executed statement\n",
    "# databases = cursor.fetchall() ## it returns a list of all databases present\n",
    "\n",
    "# databases\n",
    "\n",
    "# cursor.execute(\"SHOW TABLES\")\n",
    "\n",
    "# tables = cursor.fetchall()\n",
    "\n",
    "# tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date, headline, main_post_link, main_post_text = extract_fish_report(data_rows[1])\n",
    "date, headline, main_post_link, main_post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_fishing_report(db_obj, date, headline, post_url, post_body):\n",
    "    fishing_insert_query = (\"INSERT INTO fishing_reports \"\n",
    "                            \"(date_posted, headline, post_url, post_body) \" \n",
    "                            \"VALUES \"\n",
    "                            f'(STR_TO_DATE(\"{date}\", \"%m-%d-%Y\"), \"{headline}\", \"{post_url}\", \"{post_body}\")')\n",
    "    try:\n",
    "        cursor = db_obj.cursor()\n",
    "        cursor.execute(fishing_insert_query)\n",
    "        db_obj.commit()\n",
    "        cursor.close()\n",
    "    except Exception as e:\n",
    "        print(f\"failed to run query \\n\\n {fishing_insert_query} \\n\\n with error {e}\")\n",
    "        db_obj.rollback()\n",
    "        print(f\"failed to upload data for date {date} with headline {headline}\")\n",
    "    print(f\"Finished for date {date} with headline {headline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_fishing_report(db_obj=db, date=date, headline=headline, post_url=main_post_link, post_body=main_post_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM fishing_reports;\")\n",
    "\n",
    "## 'fetchall()' method fetches all the rows from the last executed statement\n",
    "completed_rows = cursor.fetchall()\n",
    "completed_dates = list(map(lambda x: x[1], completed_rows))\n",
    "completed_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_links = set(list(map(lambda x: x[3], completed_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_string(target_string):\n",
    "    return re.sub(r'\\W+', ' ', target_string.replace(\"'\", \"\")).strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"\n",
    "\"The weather was just spectacular\" remarked DeBuys. \\\"The best times were from noon until dark on most days, but they bit some at night, too. It looked like the fish were feeding on red crab and flying fish, from what we saw in them.  There\\'s an awful lot of good water headed up this way,\\\" continued the skipper. \\\"We saw a strong uphill current and just flew home on it. We found a one-degree edge about 290 miles down. The warm side was over 65 degrees and there were bluefin there. They looked like 25-pounders.\\\"\n",
    "\n",
    "The group caught some 19 cows, or tuna over 200 pounds. The best one weighed 242 pounds, caught by Jim Chivas of Norwalk. He said the fish fought him for 45 minutes before it was gaffed aboard the Indy. He baited a sardine on a 6/0 Mustad Demon ringed circle hook, and used 130-pound Seaguar Premier fluorocarbon and 130-pound Spectra on a new Avet 80 reel and a Super Seeker 3 X 5 rod.\n",
    "\n",
    "Dave Rocchi of Cypress won second place for a 239-pounder. He coaxed that one to the port bow after a half-hour tussle. His tuna bit a sardine on a 5/0 ringed Hayabusa hook on 100-pound Seaguar Premier fluorocarbon and 130-pound Line One spectra. He fished with an Avet HXW reel and a Calstar 665 XXH rod. He also had a 219-pound yellowfin.\n",
    "\n",
    "Bill Nelson of Fairfield won third place for a 229-pound tuna. His sardine bait was pinned on a 7/0 Mustad Demon hook and 100-pound Seaguar Premier fluorocarbon, with 130-pound Kanzen spectra backing. He used a Penn 50 reel and a Seeker 6460 XXH rod to subdue the tuna after a fight of an hour and a half.\n",
    "\n",
    "Les Nishi bagged a 224-pounder. Jim Isaac caught a brace of tuna that weighed 222 and 213 pounds. Paul Geurts had one at 219.6 pounds.\n",
    "\n",
    "Dennis Saylor of Seal Beach bagged a triple, with his fish weighing in at 207, 203 and 201 pounds. He fished sardines on 4/0 ringed Owner Super Mutu hooks on 100-pound Momoi and 130-pound Line One spectra on an Avet Raptor reel and a Calstar 6465 XH rod.\n",
    "Ev Combs of Palm Springs caught a 205-pounder with a squid under the kite. He said he used 10/0 Mustad 7691 hooks on one of the boat\\'s kite rigs: 130-pound Izorline and 130-pound Izorline spectra, an Avet 80 reel and a Super Seeker 3 X 5 rod.\n",
    "\n",
    "Chartermaster Rick Ozaki of Raider jigs and GrafTech rods got a brace, at 203 and 204 pounds. He said he baited sardines on 4/0 ringed Super Mutu hooks. He fished with 100-pound Seaguar Premier fluorocarbon and 100-pound Izorline spectra on an Avet HXW reel and a Calstar 770 XH rod.\n",
    " \n",
    "Richard Berg got one at 201.8 pounds, and Dale Lethcoe snared a 201, as did Dane Barriault. Crewman Doug Brink gaffed his own 205-pound tuna, and chef Michelle, or \\\"Frenchie,\\\" as the boys call him, caught a 220-pound yellowfin.\n",
    " \n",
    "The trip produced limits of tuna and wahoo, said skipper DeBuys. The best skin brought up to the scales was a 66-pound \\'gator caught by Jim Mann of Bonita.  \\\"He bit on an orange and purple Marauder,\\\" said Mann, \\\"and he busted it.\\\"\n",
    "\n",
    "This was the Indy\\'s last trip of the current big fish season. She will move up to 22nd St. Landing for the next several weeks, and is expected to run short trips out to San Clemente Island and other local spots.\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_string(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop over fishing report pages and scrape each (~3000 posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrape_delay = 0.5\n",
    "\n",
    "# first_page   = 25\n",
    "# highest_page = 302\n",
    "# final_page   = 182\n",
    "# for page in range(first_page, final_page + 1):\n",
    "#     if page % 5 == 0:\n",
    "#         print(f\"Starting page\", page)\n",
    "#     data_rows = get_table_rows(page_number=page)\n",
    "#     for entry in data_rows:\n",
    "#         time.sleep(scrape_delay)\n",
    "#         try:\n",
    "#             date, headline, main_post_link, main_post_text = extract_fish_report(entry)\n",
    "#             if main_post_link not in completed_links:\n",
    "#                 main_post_text = clean_string(main_post_text)\n",
    "#                 headline       = clean_string(headline)\n",
    "#                 insert_fishing_report(db_obj=db, date=date, headline=headline, post_url=main_post_link, post_body=main_post_text)\n",
    "#             else:\n",
    "#                 print(f\"Already completed for date {date}\")\n",
    "#         except Exception as e:\n",
    "#             print(\"***** ERROR ***** \\n\\n\", entry)\n",
    "#             raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what fish are being talked about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"What a trip the guys had. Yesterday we worked our way up from down south and found some more bitting Yellowtail. Finished the day with a 100 Yellows and 5 Halibut. Great action all day. To finish up the trip this morning we rock fished and had success. Giant Reds and Lingcod for everyone. Heading home. We will be in at 0530 tomorrow. Brian & the Indy crew\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(\\d+ \\w+)\", example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin collecting weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as check\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_css_selectors = {\n",
    "    \"high\":          \"tbody.ng-star-inserted:nth-child(2) > tr:nth-child(1) > td:nth-child(2)\",\n",
    "    \"low\":           \"tbody.ng-star-inserted:nth-child(2) > tr:nth-child(2) > td:nth-child(2)\",\n",
    "    \"avg\":           \"tbody.ng-star-inserted:nth-child(2) > tr:nth-child(3) > td:nth-child(2)\",\n",
    "    \"precipitation\": \"tbody.ng-star-inserted:nth-child(4) > tr:nth-child(1) > td:nth-child(2)\",\n",
    "    \"visibility\":    \"tbody.ng-star-inserted:nth-child(8) > tr:nth-child(2) > td:nth-child(2)\",\n",
    "    \"wind_max\":      \"tbody.ng-star-inserted:nth-child(8) > tr:nth-child(1) > td:nth-child(2)\",\n",
    "    \"sea_pressure\":  \"tbody.ng-star-inserted:nth-child(10) > tr:nth-child(1) > td:nth-child(2)\"\n",
    "}\n",
    "\n",
    "targets = ['low', 'avg', 'high', 'precipitation', 'visibility', 'wind_max', 'sea_pressure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_weather_reports(db_obj, date, high, low, avg, precipitation, visibility, wind_max, sea_pressure):\n",
    "    weather_insert_query = (\"INSERT INTO weather_reports \"\n",
    "                            \"(date, high_temp, low_temp, avg_temp, inches_precip, miles_visible, max_wind, sea_pressure) \" \n",
    "                            \"VALUES \"\n",
    "                            f'(STR_TO_DATE(\"{date}\", \"%Y-%m-%d\"), \"{high}\", \"{low}\", \"{avg}\", \"{precipitation}\", \"{visibility}\", \"{wind_max}\", \"{sea_pressure}\")')\n",
    "    cursor = db_obj.cursor()\n",
    "    try:\n",
    "        cursor.execute(weather_insert_query)\n",
    "        db_obj.commit()\n",
    "        print(f\"Finished for date {date}\")\n",
    "    except Exception as e:\n",
    "        print(f\"failed to run query \\n {weather_insert_query} \\n with error {e}\\n\\n\")\n",
    "        db_obj.rollback()\n",
    "        print(f\"failed to upload data for date {date}\")\n",
    "    finally:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_css_path(driver, field):\n",
    "    css_selector = weather_css_selectors[field]\n",
    "    return driver.find_element_by_css_selector(css_selector).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_weather_page(weather_url):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "\n",
    "    timeout = 15\n",
    "    with webdriver.Firefox(executable_path='/Users/mikelawrence/Downloads/geckodriver', options=options) as driver:\n",
    "        try:\n",
    "            driver.get(weather_url)\n",
    "            element_present = check.presence_of_element_located((By.CSS_SELECTOR, weather_css_selectors['high']))\n",
    "            WebDriverWait(driver, timeout).until(element_present)\n",
    "\n",
    "            weather_data = {field: float(extract_text_css_path(driver, field)) for field in targets}  \n",
    "        except Exception as e:\n",
    "            print(f\"failed to collect data for date {formatted_date}\")\n",
    "            raise e\n",
    "            \n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_code = \"KSAN\"\n",
    "year  = \"2020\"\n",
    "month = \"12\"\n",
    "day   = \"15\"\n",
    "\n",
    "formatted_date = f\"{year}-{month}-{day}\"\n",
    "formatted_date\n",
    "\n",
    "weather_history_url = f\"https://www.wunderground.com/history/daily/us/ca/san-diego/{weather_station_code}/date/{formatted_date}\"\n",
    "\n",
    "weather_data = scrape_weather_page(weather_history_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, val in weather_data.items():\n",
    "    print(f\"{f}\\t{val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM fishing_reports;\")\n",
    "\n",
    "## 'fetchall()' method fetches all the rows from the last executed statement\n",
    "completed_rows = cursor.fetchall()\n",
    "fishing_dates = set(list(map(lambda x: x[1], completed_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM weather_reports;\")\n",
    "\n",
    "## 'fetchall()' method fetches all the rows from the last executed statement\n",
    "completed_rows = cursor.fetchall()\n",
    "weather_dates = set(list(map(lambda x: x[1], completed_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_dates = fishing_dates.difference(weather_dates)\n",
    "remaining_dates = list(remaining_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "bad_dates = []\n",
    "weather_station_code = \"KSAN\"\n",
    "\n",
    "date_count = len(remaining_dates)\n",
    "\n",
    "for i, date in enumerate(remaining_dates):\n",
    "    if i % 5  == 0:\n",
    "        print(f\"Finished {i} of {date_count}\")\n",
    "    day   = date.day\n",
    "    month = date.month\n",
    "    year  = date.year\n",
    "    formatted_date = f\"{year}-{month}-{day}\"\n",
    "    weather_history_url = f\"https://www.wunderground.com/history/daily/us/ca/san-diego/{weather_station_code}/date/{formatted_date}\"\n",
    "\n",
    "    try: \n",
    "        weather_data = scrape_weather_page(weather_history_url)\n",
    "        insert_weather_reports(db, formatted_date, **weather_data)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(f\"failed to get weather data for date {formatted_date} with error {e}\")\n",
    "        bad_dates += [formatted_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join weather and fishing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('low',\n",
       " 'avg',\n",
       " 'high',\n",
       " 'precipitation',\n",
       " 'visibility',\n",
       " 'wind_max',\n",
       " 'sea_pressure')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'low', 'avg', 'high', 'precipitation', 'visibility', 'wind_max', 'sea_pressure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low',\n",
       " 'avg',\n",
       " 'high',\n",
       " 'precipitation',\n",
       " 'visibility',\n",
       " 'wind_max',\n",
       " 'sea_pressure']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather_reports.low_temp, weather_reports.avg_temp, weather_reports.high_temp, weather_reports.inches_precip, weather_reports.miles_visible, weather_reports.max_wind, weather_reports.sea_pressure'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sql_all_reports_with_weather(db_obj):\n",
    "    weather_columns = [\"low_temp\", \"avg_temp\", \"high_temp\", \n",
    "                       \"inches_precip\", \"miles_visible\", \"max_wind\", \n",
    "                       \"sea_pressure\"]\n",
    "\n",
    "    fishing_columns = [\"date_posted\", \"headline\", \"post_body\"]\n",
    "\n",
    "    weather_columns_sql = \", \".join([f\"weather_reports.{field}\" for field in weather_columns])\n",
    "    fishing_columns_sql = \", \".join([f\"fishing_reports.{field}\" for field in fishing_columns])\n",
    "\n",
    "    join_query = f\"\"\"\n",
    "    SELECT {fishing_columns_sql}, {weather_columns_sql}\n",
    "    FROM fishing_reports \n",
    "    INNER JOIN weather_reports ON weather_reports.date=fishing_reports.date_posted;\n",
    "    \"\"\"\n",
    "    return pd.read_sql(sql=join_query, con=db)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine learning sandbox",
   "language": "python",
   "name": "data-science-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
